import streamlit as st
import pickle
import google.generativeai as genai
import faiss
import numpy as np
from dotenv import load_dotenv
import os
from datetime import datetime
import time

# Load environment variables
load_dotenv()

# Page configuration
st.set_page_config(
    page_title="Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ© ÙˆØ§Ù„Ø²ÙƒÙˆÙŠØ©",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for RTL support and dark theme
st.markdown("""
<style>
    /* Dark theme */
    [data-testid="stAppViewContainer"] {
        background-color: #0e1117;
    }
    
    [data-testid="stSidebar"] {
        background-color: #262730;
    }
    
    [data-testid="stHeader"] {
        background-color: #0e1117;
    }
    
    /* Main header */
    .main-header {
        text-align: center;
        color: #4CAF50;
        border-bottom: 3px solid #4CAF50;
        padding-bottom: 15px;
        margin-bottom: 30px;
        font-size: 2rem;
        font-weight: bold;
    }
    
    /* Chat messages */
    .chat-message {
        padding: 1.2rem;
        border-radius: 10px;
        margin-bottom: 1.5rem;
        direction: rtl;
        text-align: right;
        line-height: 1.6;
    }
    
    .user-message {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        margin-left: 20%;
        box-shadow: 0 2px 5px rgba(0,0,0,0.3);
    }
    
    .assistant-message {
        background: linear-gradient(135deg, #2c3e50 0%, #34495e 100%);
        color: white;
        margin-right: 20%;
        border-left: 4px solid #4CAF50;
        box-shadow: 0 2px 5px rgba(0,0,0,0.3);
    }
    
    /* Input styling */
    .stChatInput {
        direction: rtl;
    }
    
    .stChatInput textarea {
        direction: rtl;
        text-align: right;
        background-color: #262730;
        color: white;
        border: 2px solid #4CAF50;
    }
    
    /* Sidebar styling */
    .css-1d391kg, [data-testid="stSidebar"] {
        background-color: #262730;
    }
    
    /* Buttons */
    .stButton > button {
        background-color: #4CAF50;
        color: white;
        border: none;
        border-radius: 5px;
        font-weight: bold;
    }
    
    .stButton > button:hover {
        background-color: #45a049;
        box-shadow: 0 2px 8px rgba(76, 175, 80, 0.4);
    }
    
    /* Expander */
    .streamlit-expanderHeader {
        background-color: #1e1e1e;
        color: white;
        direction: rtl;
        border-radius: 5px;
    }
    
    .streamlit-expanderContent {
        background-color: #262730;
        direction: rtl;
        text-align: right;
    }
    
    /* Text area in sidebar */
    [data-testid="stSidebar"] textarea {
        direction: rtl;
        text-align: right;
        background-color: #1e1e1e;
        color: white;
        border: 1px solid #4CAF50;
    }
    
    /* Success message */
    .stSuccess {
        background-color: #27ae60;
        color: white;
        direction: rtl;
        text-align: right;
    }
    
    /* Info boxes */
    .element-container div[data-testid="stMarkdownContainer"] p {
        direction: rtl;
        text-align: right;
    }
    
    /* Error messages */
    .error-message {
        background-color: #e74c3c;
        color: white;
        padding: 15px;
        border-radius: 8px;
        margin: 10px 0;
        direction: rtl;
        text-align: right;
    }
    
    /* Footer */
    .footer {
        text-align: center;
        color: #888;
        margin-top: 30px;
        padding-top: 20px;
        border-top: 1px solid #333;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_resource
def load_models_and_data():
    """Load the FAISS indices and chunks data"""
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        st.error("Ù…Ø·Ù„ÙˆØ¨ Ù…ÙØªØ§Ø­ API. ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ GOOGLE_API_KEY ÙÙŠ Ù…Ù„Ù .env")
        return None, None, None, None, None, None
    
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.5-pro')
    
    guides_chunks = []
    guides_index = None
    decisions_chunks = []
    decisions_index = None
    
    # Load guides
    try:
        guides_index = faiss.read_index('guides.faiss')
        with open('guides_chunks.pkl', 'rb') as f:
            guides_chunks = pickle.load(f)
    except FileNotFoundError:
        st.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙÙ‡Ø±Ø³ Ø§Ù„Ø£Ø¯Ù„Ø© Ø£Ùˆ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹.")
    
    # Load decisions
    try:
        decisions_index = faiss.read_index('decisions.faiss')
        with open('decisions_chunks.pkl', 'rb') as f:
            decisions_chunks = pickle.load(f)
    except FileNotFoundError:
        st.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙÙ‡Ø±Ø³ Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø£Ùˆ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹.")
    
    return model, guides_index, guides_chunks, decisions_index, decisions_chunks, api_key

def get_embedding(text, api_key, task_type="retrieval_query"):
    """Get embedding for the given text using Google embeddings model."""
    genai.configure(api_key=api_key)
    result = genai.embed_content(
        model="gemini-embedding-001",
        content=text,
        task_type=task_type
    )
    embedding = np.array(result['embedding'], dtype=np.float32)
    faiss.normalize_L2(embedding.reshape(1, -1))
    return embedding

def retrieve_chunks(query_embedding, index, chunks, top_k=8):
    """Retrieve top_k most similar unique chunks to the query."""
    if not index or not chunks:
        return []

    try:
        distances, indices = index.search(query_embedding.reshape(1, -1), top_k)
        retrieved_chunks = [chunks[i] for i in indices[0]]
        # Add similarity scores to the chunks for display
        for i, chunk in enumerate(retrieved_chunks):
            chunk['similarity_score'] = distances[0][i]
        return retrieved_chunks
    except Exception as e:
        st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹: {e}")
        return []

def format_decision_chunk(chunk, index):
    """Format a single decision chunk with all metadata fields"""
    metadata = chunk.get('metadata', {})
    source = metadata.get('Source_Filename', chunk.get('filename', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'))
    embedding_source = chunk.get('embedding_source', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
    
    # Build comprehensive context with ALL fields
    context_parts = [f"Ø§Ù„Ù…Ù‚Ø·Ø¹ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ {index} Ù…Ù† Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª - Ø§Ù„Ù…Ù„Ù: {source}"]
    
    # Add all metadata fields
    if 'Ø±Ù‚Ù… Ø§Ù„Ù‚Ø±Ø§Ø±' in metadata:
        context_parts.append(f"Ø±Ù‚Ù… Ø§Ù„Ù‚Ø±Ø§Ø±: {metadata['Ø±Ù‚Ù… Ø§Ù„Ù‚Ø±Ø§Ø±']}")
    
    if 'Ø±Ù‚Ù… Ø§Ù„Ø¯Ø¹ÙˆÙ‰ / Ø§Ù„Ø§Ø³ØªÙ’Ù†Ø§Ù' in metadata:
        context_parts.append(f"Ø±Ù‚Ù… Ø§Ù„Ø¯Ø¹ÙˆÙ‰ / Ø§Ù„Ø§Ø³ØªÙ’Ù†Ø§Ù: {metadata['Ø±Ù‚Ù… Ø§Ù„Ø¯Ø¹ÙˆÙ‰ / Ø§Ù„Ø§Ø³ØªÙ’Ù†Ø§Ù']}")
    
    if 'Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ø¶Ø±ÙŠØ¨ÙŠ Ø§Ù„Ù…Ø±ØªØ¨Ø· Ø¨Ø§Ù„Ù‚Ø±Ø§Ø±' in metadata:
        context_parts.append(f"Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ø¶Ø±ÙŠØ¨ÙŠ Ø§Ù„Ù…Ø±ØªØ¨Ø· Ø¨Ø§Ù„Ù‚Ø±Ø§Ø±: {metadata['Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ø¶Ø±ÙŠØ¨ÙŠ Ø§Ù„Ù…Ø±ØªØ¨Ø· Ø¨Ø§Ù„Ù‚Ø±Ø§Ø±']}")
    
    if 'Ø±Ù‚Ù… Ø§Ù„Ø¯Ø¹ÙˆÙ‡' in metadata:
        context_parts.append(f"Ø±Ù‚Ù… Ø§Ù„Ø¯Ø¹ÙˆÙ‡: {metadata['Ø±Ù‚Ù… Ø§Ù„Ø¯Ø¹ÙˆÙ‡']}")
    
    # Add the three main content fields with clear labels
    if 'Ø§Ø³Ø¨Ø§Ø¨ Ø§Ù„Ù‚Ø±Ø§Ø±' in metadata:
        context_parts.append(f"\nØ§Ø³Ø¨Ø§Ø¨ Ø§Ù„Ù‚Ø±Ø§Ø±:\n{metadata['Ø§Ø³Ø¨Ø§Ø¨ Ø§Ù„Ù‚Ø±Ø§Ø±']}")
    
    if 'Ø§Ù„Ø¨Ù†ÙˆØ¯ Ù…Ø­Ù„ Ø§Ù„Ø§Ø¹ØªØ±Ø§Ø¶' in metadata:
        context_parts.append(f"\nØ§Ù„Ø¨Ù†ÙˆØ¯ Ù…Ø­Ù„ Ø§Ù„Ø§Ø¹ØªØ±Ø§Ø¶:\n{metadata['Ø§Ù„Ø¨Ù†ÙˆØ¯ Ù…Ø­Ù„ Ø§Ù„Ø§Ø¹ØªØ±Ø§Ø¶']}")
    
    if 'Ù…Ù†Ø·ÙˆÙ‚ Ø§Ù„Ù‚Ø±Ø§Ø±' in metadata:
        context_parts.append(f"\nÙ…Ù†Ø·ÙˆÙ‚ Ø§Ù„Ù‚Ø±Ø§Ø±:\n{metadata['Ù…Ù†Ø·ÙˆÙ‚ Ø§Ù„Ù‚Ø±Ø§Ø±']}")
    
    # Add note that all fields are included
    context_parts.append(f"\n[ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø±Ø§Ø± Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ: {embedding_source}ØŒ ÙˆØ§Ù„Ù…Ù‚Ø·Ø¹ ÙŠØªØ¶Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ù‚ÙˆÙ„: Ø§Ø³Ø¨Ø§Ø¨ Ø§Ù„Ù‚Ø±Ø§Ø±ØŒ Ø§Ù„Ø¨Ù†ÙˆØ¯ Ù…Ø­Ù„ Ø§Ù„Ø§Ø¹ØªØ±Ø§Ø¶ØŒ Ù…Ù†Ø·ÙˆÙ‚ Ø§Ù„Ù‚Ø±Ø§Ø±]")
    
    return '\n'.join(context_parts)

def format_guide_chunk(chunk, index):
    """Format a single guide chunk"""
    metadata = chunk.get('metadata', {})
    source = metadata.get('filename', chunk.get('filename', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'))
    
    context_parts = [
        f"Ø§Ù„Ù…Ù‚Ø·Ø¹ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ {index} Ù…Ù† Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ø¥Ø±Ø´Ø§Ø¯ÙŠØ© - Ø§Ù„Ù…Ù„Ù: {source}",
        f"Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {chunk['text']}"
    ]
    
    return '\n'.join(context_parts)

def format_context(retrieved_chunks, source_type):
    """Format retrieved chunks into context string with sources."""
    if not retrieved_chunks:
        return ""
    
    context_parts = []
    for i, chunk in enumerate(retrieved_chunks, 1):
        if source_type == "Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª":
            context_parts.append(format_decision_chunk(chunk, i))
        else:
            context_parts.append(format_guide_chunk(chunk, i))
    
    return '\n\n'.join(context_parts)

def display_retrieved_chunks(chunks, source_type):
    """Display retrieved chunks in the sidebar"""
    if not chunks:
        st.sidebar.info(f"Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ Ù…Ù† {source_type}")
        return
    
    st.sidebar.subheader(f"Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø© Ù…Ù† {source_type}")
    
    for i, chunk in enumerate(chunks, 1):
        metadata = chunk.get('metadata', {})
        
        if source_type == "Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª":
            # For decisions, show decision number and embedding source
            decision_num = metadata.get('Ø±Ù‚Ù… Ø§Ù„Ù‚Ø±Ø§Ø±', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
            embedding_src = chunk.get('embedding_source', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
            source = metadata.get('Source_Filename', chunk.get('filename', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'))
            score = chunk.get('similarity_score', 0.0)
            
            with st.sidebar.expander(f"Ù…Ù‚Ø·Ø¹ {i} - Ù‚Ø±Ø§Ø± {decision_num} (Ù…Ù†: {embedding_src}) - Ø¯Ø±Ø¬Ø©: {score:.3f}"):
                # Show all metadata fields
                st.write(f"**Ø±Ù‚Ù… Ø§Ù„Ù‚Ø±Ø§Ø±:** {metadata.get('Ø±Ù‚Ù… Ø§Ù„Ù‚Ø±Ø§Ø±', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
                st.write(f"**Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ø¶Ø±ÙŠØ¨ÙŠ:** {metadata.get('Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ø¶Ø±ÙŠØ¨ÙŠ Ø§Ù„Ù…Ø±ØªØ¨Ø· Ø¨Ø§Ù„Ù‚Ø±Ø§Ø±', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
                st.write(f"**Ø­Ù‚Ù„ Ø§Ù„Ø¨Ø­Ø«:** {embedding_src}")
                st.write("---")
                st.text(chunk['text'][:300] + "..." if len(chunk['text']) > 300 else chunk['text'])
        else:
            # For guides
            source = metadata.get('filename', chunk.get('filename', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'))
            score = chunk.get('similarity_score', 0.0)
            
            with st.sidebar.expander(f"Ù…Ù‚Ø·Ø¹ {i} - {source} (Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {score:.3f})"):
                st.text(chunk['text'][:300] + "..." if len(chunk['text']) > 300 else chunk['text'])

def main():
    # Header
    st.markdown('<h1 class="main-header">ğŸ¤– Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ© ÙˆØ§Ù„Ø²ÙƒÙˆÙŠØ©</h1>', unsafe_allow_html=True)
    
    # Load models and data
    with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª..."):
        model, guides_index, guides_chunks, decisions_index, decisions_chunks, api_key = load_models_and_data()
    
    if model is None:
        st.error("ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬. ØªØ­Ù‚Ù‚ Ù…Ù† Ù…ÙØªØ§Ø­ API ÙˆØ§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©.")
        return
    
    # Sidebar information
    st.sidebar.header("Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…")
    st.sidebar.info(f"ğŸ“š Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ù…Ø­Ù…Ù„Ø©: {len(guides_chunks)} Ù…Ù‚Ø·Ø¹")
    st.sidebar.info(f"âš–ï¸ Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù…Ù„Ø©: {len(decisions_chunks)} Ù…Ù‚Ø·Ø¹")
    
    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    if "show_sources" not in st.session_state:
        st.session_state.show_sources = True
    
    if "top_k_guides" not in st.session_state:
        st.session_state.top_k_guides = 4
    
    if "top_k_decisions" not in st.session_state:
        st.session_state.top_k_decisions = 8
    
    # Settings
    st.sidebar.header("Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
    show_sources = st.sidebar.checkbox("Ø¹Ø±Ø¶ Ø§Ù„Ù…ØµØ§Ø¯Ø±", value=st.session_state.show_sources)
    st.session_state.show_sources = show_sources
    
    # Separate sliders for each source type
    st.sidebar.subheader("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹")
    
    top_k_guides = st.sidebar.slider(
        "ğŸ“š Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ù…Ù† Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ø¥Ø±Ø´Ø§Ø¯ÙŠØ©", 
        min_value=1, 
        max_value=10, 
        value=st.session_state.top_k_guides,
        help="Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹Ù‡Ø§ Ù…Ù† Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ø¥Ø±Ø´Ø§Ø¯ÙŠØ©"
    )
    st.session_state.top_k_guides = top_k_guides
    
    top_k_decisions = st.sidebar.slider(
        "âš–ï¸ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ù…Ù† Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª", 
        min_value=1, 
        max_value=15, 
        value=st.session_state.top_k_decisions,
        help="Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹Ù‡Ø§ Ù…Ù† Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª"
    )
    st.session_state.top_k_decisions = top_k_decisions
    
    # Display total chunks
    total_chunks = top_k_guides + top_k_decisions
    st.sidebar.info(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©: {total_chunks}")
    st.sidebar.markdown("---")
    
    # System prompt section
    st.sidebar.header("ØªØ®ØµÙŠØµ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª")
    
    # Initialize default prompt in session state
    if "system_prompt" not in st.session_state:
        st.session_state.system_prompt = """âœ¨ Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø©
1.  *Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©*: Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙ‚Ø· Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© (Ø£Ø¯Ù„Ø© Ø£Ùˆ Ù‚Ø±Ø§Ø±Ø§Øª Ø£Ùˆ ØºÙŠØ±Ù‡Ø§) Ù…Ø¹ ØªØµØ­ÙŠØ­ Ø£ÙŠ Ø£Ø®Ø·Ø§Ø¡ ÙƒØªØ§Ø¨ÙŠØ© Ø¥Ù† ÙˆØ¬Ø¯Øª.
2.  *ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨*:
    ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø¯Ù‚Ø©:
    Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø£ÙˆÙ„:
    Ø§Ø³Ù… Ø§Ù„Ù…ØµØ¯Ø±: [Ø§Ø³Ù… Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø£Ùˆ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ù…Ø¹ Ø±Ù‚Ù… Ø§Ù„ØµÙØ­Ø© Ø§Ù„ØªÙŠ ØªØªØ¶Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø©ØŒ Ø£Ùˆ ÙÙŠ Ø­Ø§Ù„ ÙƒØ§Ù† Ù‚Ø±Ø§Ø± Ø§Ø³ØªØ®Ø±Ø¬ Ø±Ù‚Ù… Ø§Ù„Ù‚Ø±Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù…Ù† Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…ØµØ¯Ø± Ù…Ø¹ Ø§Ø³Ù… Ø§Ù„Ø¯Ø§Ø¦Ø±Ø© Ø§Ù„Ù…ØµØ¯Ø±Ø© Ù„Ù‡ ÙˆØªØ¬Ø§Ù‡Ù„ Ø±Ù‚Ù… Ø§Ù„Ø¯Ø¹ÙˆÙ‰]
    Ù…Ù„Ø®Øµ Ù…Ø§ Ø¬Ø§Ø¡ ÙÙŠ Ø§Ù„Ù…ØµØ¯Ø±: [Ø´Ø±Ø­ Ù…Ù„Ø®Øµ Ù„Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©]
    Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø±: [Ø§Ù„Ø±Ø§Ø¨Ø· Ø¥Ù† ÙˆØ¬Ø¯ØŒ Ø£Ùˆ "ØºÙŠØ± Ù…ØªÙˆÙØ±"]
    [ÙˆÙ‡ÙƒØ°Ø§ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø±]
3.  *ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ØµØ§Ø¯Ø±*: Ø§Ø¨Ø¯Ø£ Ø¨Ø§Ù„Ø£Ø¯Ù„Ø© Ø«Ù… Ø§Ù„Ù‚Ø±Ø§Ø±Ø§ØªØŒ ÙˆØ±ØªØ¨Ù‡Ù… Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©.
4.  *Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ*: ÙÙŠ Ù‚Ø³Ù… "Ù…Ù„Ø®Øµ Ù…Ø§ Ø¬Ø§Ø¡ ÙÙŠ Ø§Ù„Ù…ØµØ¯Ø±" Ø§Ø´Ø±Ø­ Ø¨Ø§Ù„ØªÙØµÙŠÙ„:
ÙÙŠ Ø­Ø§Ù„ ÙƒØ§Ù† Ø§Ù„Ù…ØµØ¯Ø± Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ø¯Ù„ÙŠÙ„ Ø¥Ø±Ø´Ø§Ø¯ÙŠ Ø£Ø´Ø±Ø­ ÙƒØ§Ù„ØªØ§Ù„ÙŠ:
* Ø´Ø±Ø­ ØªÙØµÙŠÙ„ÙŠ Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„ØŒ Ù…Ø¹ Ø¹Ø¯Ù… Ø°ÙƒØ± Ø£ÙŠ Ù…Ø¨Ø§Ù„Øº ØªØªØ¹Ù„Ù‚ Ø¨Ø£Ù…Ø«Ù„Ø© Ù…Ø°ÙƒÙˆØ±Ø© ÙÙŠ Ø§Ù„Ù…ØµØ¯Ø±.    
ÙÙŠ Ø­Ø§Ù„ ÙƒØ§Ù† Ø§Ù„Ù…ØµØ¯Ø± Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù‚Ø±Ø§Ø± ÙÙŠ Ø¯Ø¹ÙˆÙ‰ Ø£Ø´Ø±Ø­ ÙƒØ§Ù„ØªØ§Ù„ÙŠ:
* Ù‚Ù… Ø¨Ø³Ø±Ø¯ ØªÙØµÙŠÙ„ Ù„Ù‚Ø±Ø§Ø± (Ø§Ù„Ù„Ø¬Ù†Ø© Ø§Ù„Ø§Ø¨ØªØ¯Ø§Ø¦ÙŠØ©) ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© ÙˆØ£Ø°ÙƒØ± Ø§Ø³Ù… Ø§Ù„Ù„Ø¬Ù†Ø© Ù…Ù† ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù‚Ø±Ø§Ø±.
* Ù‚Ù… Ø¨Ø³Ø±Ø¯ ØªÙØµÙŠÙ„ Ù„Ù‚Ø±Ø§Ø± (Ø§Ù„Ù„Ø¬Ù†Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©) ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© ÙˆØ£Ø°ÙƒØ± Ø§Ø³Ù… Ø§Ù„Ù„Ø¬Ù†Ø© Ù…Ù† ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù‚Ø±Ø§Ø±.
**Ø­Ø§ÙˆÙ„ ÙÙŠ Ø§Ù„Ø³Ø±Ø¯ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø´Ø±Ø­ Ù…Ø­Ø¯Ø¯ ÙÙ‚Ø· Ø¨Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¯ÙˆÙ† Ø§Ù„Ø®Ø±ÙˆØ¬ Ø£Ùˆ Ø¥Ø¹Ø·Ø§Ø¡ Ù†ØªÙŠØ¬Ø© Ø¹Ø§Ù…Ø© Ø£Ùˆ Ù†ØªÙŠØ¬Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…ØµØ§Ø¯Ø±ØŒ Ù…Ø¹ Ø¹Ø¯Ù… Ø°ÙƒØ± Ø£ÙŠ Ù…Ø¨Ø§Ù„Øº ØªØ±ØªØ¨Ø· Ø¨Ø§Ù„Ù‚Ø±Ø§Ø±.
**Ù‚Ù… Ø¨Ø¹Ø±Ø¶ Ù…ØµØ§Ø¯Ø± Ø¥Ø¶Ø§ÙÙŠØ© Ø¨Ù†ÙØ³ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ ØªØªØ¶Ù…Ù† Ø¨Ù†ÙˆØ¯ Ù…Ø´Ø§Ø¨Ù‡Ù‡ Ù„Ù„Ø­Ø§Ù„Ø© Ø¥Ù† ØªÙˆÙØ±Øª Ø­ØªÙ‰ ÙˆØ¥Ù† ÙƒØ§Ù†Øª Ù„ÙŠØ³Øª Ø¨Ù†ÙØ³ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶ ÙÙŠ Ø§Ù„Ø³Ø¤Ø§Ù„.
5.  *ØºÙŠØ§Ø¨ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª*: Ø¥Ø°Ø§ Ù„Ù… ØªØ­ØªÙˆÙ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© ÙƒØ§ÙÙŠØ©ØŒ Ø§ÙƒØªØ¨:
    > "ÙŠØ¸Ù‡Ø± Ø£Ù† Ø§Ù„Ø¨Ù†Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù†Ù‡ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§ØªØŒ Ø­Ø§ÙˆÙ„ Ø£Ù† ØªØ¬Ø±Ø¨ ØªÙØµÙŠÙ„ Ø¢Ø®Ø± Ø£Ùˆ Ù…Ø³Ù…Ù‰ Ø¢Ø®Ø± Ù„Ù„Ø¨Ù†Ø¯"
6.  *Ø§Ù„Ù„ØºØ©*: Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·.
7.  *Ø§Ù„ØªÙØµÙŠÙ„*: Ø§Ø´Ø±Ø­ ÙƒÙ„ Ù…ØµØ¯Ø± Ø¹Ù„Ù‰ Ø­Ø¯Ø©.
8.  *Ø§Ù„ØªÙ†Ø³ÙŠÙ‚*: Ù‚Ù… Ø¨Ø¹Ø±Ø¶ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ: 
Ø§Ù„ØµÙ Ø§Ù„Ø£ÙˆÙ„: Ø±Ù‚Ù… Ø§Ù„Ù…ØµØ¯Ø±.
Ø§Ù„ØµÙ Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø§Ø³Ù… Ø§Ù„Ù…ØµØ¯Ø±.
Ø§Ù„ØµÙ Ø§Ù„Ø«Ø§Ù„Ø«: Ù…Ù„Ø®Øµ Ù…Ø§ Ø¬Ø§Ø¡ ÙÙŠ Ø§Ù„Ù…ØµØ¯Ø± Ø¨ØªÙ†Ø³ÙŠÙ‚ Ø¬Ø°Ø§Ø¨ ÙˆÙ…Ø±ØªØ¨.
Ø§Ù„ØµÙ Ø§Ù„Ø£Ø®ÙŠØ±: Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø±"""
    
    # Create expandable section for prompt editing
    with st.sidebar.expander("âœï¸ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ©", expanded=False):
        custom_prompt = st.text_area(
            "Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ©:",
            value=st.session_state.system_prompt,
            height=300,
            help="Ù‚Ù… Ø¨ØªØ®ØµÙŠØµ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„ØªÙŠ ÙŠØªØ¨Ø¹Ù‡Ø§ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©"
        )
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ’¾ Ø­ÙØ¸", use_container_width=True):
                st.session_state.system_prompt = custom_prompt
                st.success("ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª!")
        
        with col2:
            if st.button("ğŸ”„ Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ", use_container_width=True):
                st.session_state.system_prompt = """âœ¨ Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø©
1.  *Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©*: Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙ‚Ø· Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© (Ø£Ø¯Ù„Ø© Ø£Ùˆ Ù‚Ø±Ø§Ø±Ø§Øª Ø£Ùˆ ØºÙŠØ±Ù‡Ø§) Ù…Ø¹ ØªØµØ­ÙŠØ­ Ø£ÙŠ Ø£Ø®Ø·Ø§Ø¡ ÙƒØªØ§Ø¨ÙŠØ© Ø¥Ù† ÙˆØ¬Ø¯Øª.
2.  *ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨*:
    ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø¯Ù‚Ø©:
    Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø£ÙˆÙ„:
    Ø§Ø³Ù… Ø§Ù„Ù…ØµØ¯Ø±: [Ø§Ø³Ù… Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø£Ùˆ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ù…Ø¹ Ø±Ù‚Ù… Ø§Ù„ØµÙØ­Ø© Ø§Ù„ØªÙŠ ØªØªØ¶Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø©ØŒ Ø£Ùˆ ÙÙŠ Ø­Ø§Ù„ ÙƒØ§Ù† Ù‚Ø±Ø§Ø± Ø§Ø³ØªØ®Ø±Ø¬ Ø±Ù‚Ù… Ø§Ù„Ù‚Ø±Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù…Ù† Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…ØµØ¯Ø± Ù…Ø¹ Ø§Ø³Ù… Ø§Ù„Ø¯Ø§Ø¦Ø±Ø© Ø§Ù„Ù…ØµØ¯Ø±Ø© Ù„Ù‡ ÙˆØªØ¬Ø§Ù‡Ù„ Ø±Ù‚Ù… Ø§Ù„Ø¯Ø¹ÙˆÙ‰]
    Ù…Ù„Ø®Øµ Ù…Ø§ Ø¬Ø§Ø¡ ÙÙŠ Ø§Ù„Ù…ØµØ¯Ø±: [Ø´Ø±Ø­ Ù…Ù„Ø®Øµ Ù„Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©]
    Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø±: [Ø§Ù„Ø±Ø§Ø¨Ø· Ø¥Ù† ÙˆØ¬Ø¯ØŒ Ø£Ùˆ "ØºÙŠØ± Ù…ØªÙˆÙØ±"]
    [ÙˆÙ‡ÙƒØ°Ø§ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø±]
3.  *ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ØµØ§Ø¯Ø±*: Ø§Ø¨Ø¯Ø£ Ø¨Ø§Ù„Ø£Ø¯Ù„Ø© Ø«Ù… Ø§Ù„Ù‚Ø±Ø§Ø±Ø§ØªØŒ ÙˆØ±ØªØ¨Ù‡Ù… Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©.
4.  *Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ*: ÙÙŠ Ù‚Ø³Ù… "Ù…Ù„Ø®Øµ Ù…Ø§ Ø¬Ø§Ø¡ ÙÙŠ Ø§Ù„Ù…ØµØ¯Ø±" Ø§Ø´Ø±Ø­ Ø¨Ø§Ù„ØªÙØµÙŠÙ„:
ÙÙŠ Ø­Ø§Ù„ ÙƒØ§Ù† Ø§Ù„Ù…ØµØ¯Ø± Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ø¯Ù„ÙŠÙ„ Ø¥Ø±Ø´Ø§Ø¯ÙŠ Ø£Ø´Ø±Ø­ ÙƒØ§Ù„ØªØ§Ù„ÙŠ:
* Ø´Ø±Ø­ ØªÙØµÙŠÙ„ÙŠ Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„ØŒ Ù…Ø¹ Ø¹Ø¯Ù… Ø°ÙƒØ± Ø£ÙŠ Ù…Ø¨Ø§Ù„Øº ØªØªØ¹Ù„Ù‚ Ø¨Ø£Ù…Ø«Ù„Ø© Ù…Ø°ÙƒÙˆØ±Ø© ÙÙŠ Ø§Ù„Ù…ØµØ¯Ø±.    
ÙÙŠ Ø­Ø§Ù„ ÙƒØ§Ù† Ø§Ù„Ù…ØµØ¯Ø± Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù‚Ø±Ø§Ø± ÙÙŠ Ø¯Ø¹ÙˆÙ‰ Ø£Ø´Ø±Ø­ ÙƒØ§Ù„ØªØ§Ù„ÙŠ:
* Ù‚Ù… Ø¨Ø³Ø±Ø¯ ØªÙØµÙŠÙ„ Ù„Ù‚Ø±Ø§Ø± (Ø§Ù„Ù„Ø¬Ù†Ø© Ø§Ù„Ø§Ø¨ØªØ¯Ø§Ø¦ÙŠØ©) ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© ÙˆØ£Ø°ÙƒØ± Ø§Ø³Ù… Ø§Ù„Ù„Ø¬Ù†Ø© Ù…Ù† ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù‚Ø±Ø§Ø±.
* Ù‚Ù… Ø¨Ø³Ø±Ø¯ ØªÙØµÙŠÙ„ Ù„Ù‚Ø±Ø§Ø± (Ø§Ù„Ù„Ø¬Ù†Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©) ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© ÙˆØ£Ø°ÙƒØ± Ø§Ø³Ù… Ø§Ù„Ù„Ø¬Ù†Ø© Ù…Ù† ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù‚Ø±Ø§Ø±.
**Ø­Ø§ÙˆÙ„ ÙÙŠ Ø§Ù„Ø³Ø±Ø¯ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø´Ø±Ø­ Ù…Ø­Ø¯Ø¯ ÙÙ‚Ø· Ø¨Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¯ÙˆÙ† Ø§Ù„Ø®Ø±ÙˆØ¬ Ø£Ùˆ Ø¥Ø¹Ø·Ø§Ø¡ Ù†ØªÙŠØ¬Ø© Ø¹Ø§Ù…Ø© Ø£Ùˆ Ù†ØªÙŠØ¬Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…ØµØ§Ø¯Ø±ØŒ Ù…Ø¹ Ø¹Ø¯Ù… Ø°ÙƒØ± Ø£ÙŠ Ù…Ø¨Ø§Ù„Øº ØªØ±ØªØ¨Ø· Ø¨Ø§Ù„Ù‚Ø±Ø§Ø±.
**Ù‚Ù… Ø¨Ø¹Ø±Ø¶ Ù…ØµØ§Ø¯Ø± Ø¥Ø¶Ø§ÙÙŠØ© Ø¨Ù†ÙØ³ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ ØªØªØ¶Ù…Ù† Ø¨Ù†ÙˆØ¯ Ù…Ø´Ø§Ø¨Ù‡Ù‡ Ù„Ù„Ø­Ø§Ù„Ø© Ø¥Ù† ØªÙˆÙØ±Øª Ø­ØªÙ‰ ÙˆØ¥Ù† ÙƒØ§Ù†Øª Ù„ÙŠØ³Øª Ø¨Ù†ÙØ³ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶ ÙÙŠ Ø§Ù„Ø³Ø¤Ø§Ù„.
5.  *ØºÙŠØ§Ø¨ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª*: Ø¥Ø°Ø§ Ù„Ù… ØªØ­ØªÙˆÙ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© ÙƒØ§ÙÙŠØ©ØŒ Ø§ÙƒØªØ¨:
    > "ÙŠØ¸Ù‡Ø± Ø£Ù† Ø§Ù„Ø¨Ù†Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù†Ù‡ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§ØªØŒ Ø­Ø§ÙˆÙ„ Ø£Ù† ØªØ¬Ø±Ø¨ ØªÙØµÙŠÙ„ Ø¢Ø®Ø± Ø£Ùˆ Ù…Ø³Ù…Ù‰ Ø¢Ø®Ø± Ù„Ù„Ø¨Ù†Ø¯"
6.  *Ø§Ù„Ù„ØºØ©*: Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·.
7.  *Ø§Ù„ØªÙØµÙŠÙ„*: Ø§Ø´Ø±Ø­ ÙƒÙ„ Ù…ØµØ¯Ø± Ø¹Ù„Ù‰ Ø­Ø¯Ø©.
8.  *Ø§Ù„ØªÙ†Ø³ÙŠÙ‚*: Ù‚Ù… Ø¨Ø¹Ø±Ø¶ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ: 
Ø§Ù„ØµÙ Ø§Ù„Ø£ÙˆÙ„: Ø±Ù‚Ù… Ø§Ù„Ù…ØµØ¯Ø±.
Ø§Ù„ØµÙ Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø§Ø³Ù… Ø§Ù„Ù…ØµØ¯Ø±.
Ø§Ù„ØµÙ Ø§Ù„Ø«Ø§Ù„Ø«: Ù…Ù„Ø®Øµ Ù…Ø§ Ø¬Ø§Ø¡ ÙÙŠ Ø§Ù„Ù…ØµØ¯Ø± Ø¨ØªÙ†Ø³ÙŠÙ‚ Ø¬Ø°Ø§Ø¨ ÙˆÙ…Ø±ØªØ¨.
Ø§Ù„ØµÙ Ø§Ù„Ø£Ø®ÙŠØ±: Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø±"""
                st.rerun()
    
    # Clear chat button
    if st.sidebar.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©", use_container_width=True):
        st.session_state.messages = []
        st.rerun()
    
    # Display chat history
    for message in st.session_state.messages:
        if message["role"] == "user":
            st.markdown(f'<div class="chat-message user-message"><strong>Ø£Ù†Øª:</strong> {message["content"]}</div>', unsafe_allow_html=True)
        else:
            st.markdown(f'<div class="chat-message assistant-message"><strong>Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯:</strong> {message["content"]}</div>', unsafe_allow_html=True)
    
    # Chat input
    query = st.chat_input("Ø§Ø³Ø£Ù„ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ© ÙˆØ§Ù„Ø²ÙƒÙˆÙŠØ©...")
    
    if query:
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": query})
        
        # Display user message immediately
        st.markdown(f'<div class="chat-message user-message"><strong>Ø£Ù†Øª:</strong> {query}</div>', unsafe_allow_html=True)
        
        try:
            with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª..."):
                # Get query embedding
                q_embedding = get_embedding(query, api_key)
                
                # Retrieve chunks
                guide_retrieved = []
                if guides_index and len(guides_chunks) > 0:
                    guide_retrieved = retrieve_chunks(q_embedding, guides_index, guides_chunks, top_k=st.session_state.top_k_guides)
                
                decision_retrieved = []
                if decisions_index and len(decisions_chunks) > 0:
                    decision_retrieved = retrieve_chunks(q_embedding, decisions_index, decisions_chunks, top_k=st.session_state.top_k_decisions)
                
                # Display retrieved chunks in sidebar if enabled
                if show_sources:
                    st.sidebar.markdown("---")
                    st.sidebar.subheader("Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø© Ù„Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ")
                    display_retrieved_chunks(guide_retrieved, "Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ø¥Ø±Ø´Ø§Ø¯ÙŠØ©")
                    display_retrieved_chunks(decision_retrieved, "Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª")
                
                # Format context with ALL metadata
                guides_context = format_context(guide_retrieved, "Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ø¥Ø±Ø´Ø§Ø¯ÙŠØ©")
                decisions_context = format_context(decision_retrieved, "Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª")
                full_context = f"Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ø¥Ø±Ø´Ø§Ø¯ÙŠØ©:\n{guides_context}\n\nØ§Ù„Ù‚Ø±Ø§Ø±Ø§Øª:\n{decisions_context}".strip()
                
                # Debug: print to console to verify all fields are included
                print("=" * 80)
                print("DECISIONS CONTEXT WITH ALL METADATA:")
                print(decisions_context)
                print("=" * 80)
                
                if not full_context:
                    response = "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù‚Ø§Ø·Ø¹ Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø³Ø¤Ø§Ù„Ùƒ."
                else:
                    # Create prompt using the customizable system prompt
                    prompt = f"""Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ÙÙ‡Ù… ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ© ÙˆØ§Ù„Ø²ÙƒÙˆÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.
ÙŠØ¬Ø¨ Ø£Ù† ØªØ¹ØªÙ…Ø¯ Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ø£Ø¯Ù†Ø§Ù‡.

{st.session_state.system_prompt}

Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©:
{full_context}

Ø§Ù„Ø³Ø¤Ø§Ù„: {query}

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"""
                    
                    # Generate response
                    response = model.generate_content(prompt).text
            
            # Display assistant response
            st.markdown(f'<div class="chat-message assistant-message"><strong>Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯:</strong> {response}</div>', unsafe_allow_html=True)
            
            # Add assistant message to chat history
            st.session_state.messages.append({"role": "assistant", "content": response})
            
        except Exception as e:
            error_message = f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {str(e)}"
            st.markdown(f'<div class="error-message">{error_message}</div>', unsafe_allow_html=True)
            st.session_state.messages.append({"role": "assistant", "content": error_message})
    
    # Footer
    st.markdown("---")
    st.markdown(
        '<div class="footer">'
        'ğŸ¤– Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ© ÙˆØ§Ù„Ø²ÙƒÙˆÙŠØ© - Ù…Ø¯Ø¹ÙˆÙ… Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ'
        '</div>', 
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()